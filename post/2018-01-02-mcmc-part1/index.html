<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.31.1" />
  <meta name="author" content="Justin A. Ellis">
  <meta name="description" content="Astrophysicist and  Data Scientist">

  
  <link rel="alternate" hreflang="en-us" href="https://jellis18.github.io/post/2018-01-02-mcmc-part1/">

  
  


  

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/xcode.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-109904120-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://jellis18.github.io/post/2018-01-02-mcmc-part1/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@justinellis18">
  <meta property="twitter:creator" content="@justinellis18">
  
  <meta property="og:site_name" content="Justin A. Ellis">
  <meta property="og:url" content="https://jellis18.github.io/post/2018-01-02-mcmc-part1/">
  <meta property="og:title" content="A Practical Guide to MCMC Part 1: MCMC Basics | Justin A. Ellis">
  <meta property="og:description" content="">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-01-03T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-01-05T00:00:00&#43;00:00">
  

  

  <title>A Practical Guide to MCMC Part 1: MCMC Basics | Justin A. Ellis</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Justin A. Ellis</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Portfolio</span>
            
          </a>
        </li>

        
        

        

        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">A Practical Guide to MCMC Part 1: MCMC Basics</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
        Last updated on
    
    <time datetime="2018-01-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Jan 5, 2018
    </time>
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    15 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://jellis18.github.io/post/2018-01-02-mcmc-part1/#disqus_thread"></a>
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=A%20Practical%20Guide%20to%20MCMC%20Part%201%3a%20MCMC%20Basics&amp;url=https%3a%2f%2fjellis18.github.io%2fpost%2f2018-01-02-mcmc-part1%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fjellis18.github.io%2fpost%2f2018-01-02-mcmc-part1%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjellis18.github.io%2fpost%2f2018-01-02-mcmc-part1%2f&amp;title=A%20Practical%20Guide%20to%20MCMC%20Part%201%3a%20MCMC%20Basics"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fjellis18.github.io%2fpost%2f2018-01-02-mcmc-part1%2f&amp;title=A%20Practical%20Guide%20to%20MCMC%20Part%201%3a%20MCMC%20Basics"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=A%20Practical%20Guide%20to%20MCMC%20Part%201%3a%20MCMC%20Basics&amp;body=https%3a%2f%2fjellis18.github.io%2fpost%2f2018-01-02-mcmc-part1%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        <p>Markov Chain Monte-Carlo (MCMC) is an art, pure and simple. Throughout my career I have learned several tricks and techniques from various &quot;artists&quot; of MCMC. In this guide I hope to impart some of that knowledge to newcomers to MCMC while at the same time learning/teaching about proper and pythonic code design. I also hope that this will truly be a practical (i.e. little theoretical statistics knowledge needed) guide to MCMC. By that, I mean that we will treat MCMC as a <em>tool</em> not an area of statistical theory to study. To that end we will forego most of the theoretical underpinnings of MCMCs and skip straight to implementation and optimization.</p>

<h1 id="overview">Overview</h1>

<p>In this post you will:</p>

<ol>
<li>Get a brief introduction to MCMC techniques</li>
<li>Understand and visualize the Metropolis-Hastings algorithm</li>
<li>Implement a Metropolis-Hastings MCMC sampler from scratch</li>
<li>Learn about basic MCMC diagnostics</li>
<li>Run your MCMC and push its limits on various examples</li>
</ol>

<div class="alert alert-note">
  <p>All material in this post is generated from a Jupyter notebook that can be downloaded
<a href="https://github.com/jellis18/practical_mcmc/blob/master/nbs/part1.ipynb">here</a>.</p>

</div>


<h1 id="introduction">Introduction</h1>

<p>MCMCs are used to map out and sample probability distribution or probability density functions. The problem of performing probabilistic inference and fitting models to data are ubiquitous in many areas of science, statistics, economics, and business. In short, if you have a probabilistic model with unknown parameters, you will need MCMC (or similar techniques) to obtain probability distributions of those unknown parameters.</p>

<p>Now, there is a <em>huge</em> ecosystem of MCMC variants: Metropolis-Hastings sampling, Gibbs Sampling, ensemble sampling, parallel tempering, adaptive MCMC, Hamiltonian Monte-Carlo, Reversible Jump MCMC, etc, but don't panic yet; we will start with a basic Metropolis-Hastings sampler and gradually build (in other posts) to more sophisticated techniques. All of the techniques listed above can be quite useful in certain situations (and sometimes extremely tricky to implement: I'm looking at you Reversible Jump!) but I have found that simple Metropolis-Hastings with a bit of adaptation can handle a large array of high dimensional complex problems.</p>

<p>In this first post we will build a simple Metropolis Hastings sampler. Before we get to the specifics of this algorithm we need to lay down a few ground rules for MCMCs:</p>

<ol>
<li>MCMCs stochastically explore the parameter space in such a way that the histogram of their samples produces the target distribution.</li>
<li>Markovian: Evolution of the chain (i.e., collections of samples from one iteration to the other) only depends on the current position and some transition probability distribution (i.e., how we move from one point in parameter space to another). This means that the chain has no memory and past samples cannot be used to determine new positions in parameter space.</li>
<li>The chain will converge to the target distribution if the transition probability is:

<ul>
<li><em>irreducible</em>: From any point in parameter space, we must be able to reach any other point in the space in a finite number of steps.<br></li>
<li><em>positive recurrent</em>: For any point in parameter space, the expected number of steps for the chain to return to that point is finite. This means that the chain must be able to re-visit previously explored areas of parameter space.</li>
<li><em>aperiodic</em>: The number of steps to return to a given point must not be a multiple of some value <span  class="math">\(k\)</span>. This means that the chain cannot get caught in cycles.</li>
</ul></li>
</ol>

<p>Fear not, this is the most technical thing that we will cover and all MCMC methods will have these rules built in!</p>

<h1 id="metropolis-hastings-mcmc">Metropolis Hastings MCMC</h1>

<p>Suppose we have a target posterior distribution <span  class="math">\(\pi(x)\)</span>, where <span  class="math">\(x\)</span> here can be any collection of parameters (not a single parameter). In order to move around this parameter space we must formulate some proposal distribution <span  class="math">\(q(x_{i+1}|x_i)\)</span>, that specifies the probability of moving to a point in parameter space, <span  class="math">\(x_{i+1}\)</span> given that we are currently at <span  class="math">\(x_i\)</span>. The Metropolis Hastings algorithm accepts a &quot;jump&quot; to <span  class="math">\(x_{i+1}\)</span> with the following probability</p>

<p><span  class="math">\[
\kappa(x_{i+1}|x_i) = \mathrm{min}\left(1, \frac{\pi(x_{i+1})q(x_i|x_{i+1})}{\pi(x_{i})q(x_{i+1}|x_{i})}\right) = \mathrm{min}(1, H),
\]</span></p>

<p>where the fraction above is called the Hastings ratio, <span  class="math">\(H\)</span>. What the above expression means is that the probability of transitioning from point <span  class="math">\(x_{i+1}\)</span> given the current position <span  class="math">\(x_i\)</span> is a function of the ratio of the value of the posterior at the new point to the old point (i.e., <span  class="math">\(\pi(x_{i+1})/\pi(x_i)\)</span>) and the ratio of the transition probabilities at the new point to the old point (i.e. <span  class="math">\(q(x_i|x_{i+1})/q(x_{i+1}|x_i)\)</span>). Firstly, it is clear that if this ratio is <span  class="math">\(>\)</span> 1 then the jump will be accepted (i.e. the chain advances to <span  class="math">\(x_{i+1}\)</span>). Secondly, the ratio of the target posteriors ensures that the chain will gradually move to high probability regions. Lastly, the ratio of the transition probabilities ensures that the chain is not influenced by &quot;favored&quot; locations in the proposal distribution function. If this last point is confusing, never worry, we will give an example of this later. For now, rejoice in the fact that many proposal distributions are symmetric (i.e., <span  class="math">\(q(x_{i+1}|x_i) = q(x_i|x_{i+1})\)</span>).</p>

<p>The metropolis hastings algorithm is then:</p>

<ol>
<li>Initialize parameters <span  class="math">\(x_0\)</span>.</li>
<li>for <span  class="math">\(i=1\)</span> to <span  class="math">\(i=N\)</span>:

<ul>
<li>Generate proposed parameters: <span  class="math">\(x_* \sim q(x_*|x_i)\)</span></li>
<li>Sample from uniform distribution: <span  class="math">\(u\sim U(0, 1)\)</span></li>
<li>Compute Hastings ratio: <span  class="math">\(H=\frac{\pi(x_{*})q(x_i|x_{*})}{\pi(x_{i})q(x_{*}|x_{i})}\)</span></li>
<li>if <span  class="math">\(u < \mathrm{min}(1,H)\)</span> then <span  class="math">\(x_{i+1}=x_*\)</span></li>
<li>else <span  class="math">\(x_{i+1} = x_i\)</span></li>
</ul></li>
</ol>

<p>For each step in this loop, we draw a proposed parameters <span  class="math">\(x_*\)</span>. We then compute the Hastings ratio using this proposed point. By drawing the number <span  class="math">\(u\sim U(0,1)\)</span> we allow for a &quot;jump&quot; to <span  class="math">\(x_*\)</span> with probability <span  class="math">\(\mathrm{min}(1, H)\)</span>. So if <span  class="math">\(u < \mathrm{min}(1,H)\)</span> then the jump is accepted then we advance the chain <span  class="math">\(x_{i+1}=x_*\)</span>, if it is not then we stay at the current position <span  class="math">\(x_i\)</span>. If the proposal distribution <span  class="math">\(q\)</span> obeys the three rules above (irreducible, positive recurrent, and aperiodic) then this algorithm is guaranteed to work. However, choosing smart proposal distributions is what makes MCMC an art!</p>

<p>Before we get into that, lets write some code for a very simple but illustrative example. By the way, if you are new to this then write this code yourself, don't just read it. Writing your own simple MCMC code is the best way to learn and understand.</p>

<h1 id="simple-metropolis-hastings-sampler">Simple Metropolis Hastings sampler</h1>

<p>The best way to learn is to do. So lets make a simple metropolis hastings sampler. The code below implements the MH-sampler through a function <code>mh_sampler</code>. This function is simple, but generic enough to work with different kinds of posterior distributions and different kinds of jump proposals</p>

<pre><code class="language-python">def mh_sampler(x0, lnprob_fn, prop_fn, prop_fn_kwargs={}, iterations=100000):
    &quot;&quot;&quot;Simple metropolis hastings sampler.

    :param x0: Initial array of parameters.
    :param lnprob_fn: Function to compute log-posterior.
    :param prop_fn: Function to perform jumps.
    :param prop_fn_kwargs: Keyword arguments for proposal function
    :param iterations: Number of iterations to run sampler. Default=100000

    :returns:
        (chain, acceptance, lnprob) tuple of parameter chain , acceptance rate
        and log-posterior chain.
    &quot;&quot;&quot;

    # number of dimensions
    ndim = len(x0)

    # initialize chain, acceptance rate and lnprob
    chain = np.zeros((iterations, ndim))
    lnprob = np.zeros(iterations)
    accept_rate = np.zeros(iterations)

    # first samples
    chain[0] = x0
    lnprob0 = lnprob_fn(x0)
    lnprob[0] = lnprob0

    # start loop
    naccept = 0
    for ii in range(1, iterations):

        # propose
        x_star, factor = prop_fn(x0, **prop_fn_kwargs)

        # draw random uniform number
        u = np.random.uniform(0, 1)

        # compute hastings ratio
        lnprob_star = lnprob_fn(x_star)
        H = np.exp(lnprob_star - lnprob0) * factor

        # accept/reject step (update acceptance counter)
        if u &lt; H:
            x0 = x_star
            lnprob0 = lnprob_star
            naccept += 1

        # update chain
        chain[ii] = x0
        lnprob[ii] = lnprob0
        accept_rate[ii] = naccept / ii

    return chain, accept_rate, lnprob
</code></pre>

<p>The function follows the MH-algorithm exactly as written above. It will return the chain samples (i.e. the columns are the parameters and the rows are the sampler iterations), acceptance rate per iteration and log-posterior values per iteration. Generally, this is all the information you need to produce inferences and check convergence and efficiency.</p>

<p>So, now we have a MH-sampler function but we don't yet have a proposal distribution. The simplest proposal distribution is just a draw from the prior distribution; however, this is hugely inefficient, and since this is a <em>practical</em> guide lets do something useful. Next to a prior draw, a Gaussian proposal with fixed standard deviation is the simplest proposal. In fact, nearly all other sophisticated proposal distributions are still a Gaussian distribution, albeit without a constant standard deviation / covariance. We will cover these in the next post but for now lets make a simple distribution.</p>

<pre><code class="language-python">def gaussian_proposal(x, sigma=0.1):
    &quot;&quot;&quot;
    Gaussian proposal distribution.

    Draw new parameters from Gaussian distribution with
    mean at current position and standard deviation sigma.

    Since the mean is the current position and the standard
    deviation is fixed. This proposal is symmetric so the ratio
    of proposal densities is 1.

    :param x: Parameter array
    :param sigma:
        Standard deviation of Gaussian distribution. Can be scalar
        or vector of length(x)

    :returns: (new parameters, ratio of proposal densities)
    &quot;&quot;&quot;

    # Draw x_star
    x_star = x + np.random.randn(len(x)) * sigma

    # proposal ratio factor is 1 since jump is symmetric
    qxx = 1

    return (x_star, qxx)
</code></pre>

<p>This proposal is nearly as simple as it gets, mathematically it is:</p>

<p><span  class="math">\[
q(x_*|x_i) = \textrm{Normal}(x_i, \sigma^2),
\]</span></p>

<p>that is, a Gaussian centered on the current position <span  class="math">\(x_i\)</span> with variance given by a standard deviation parameter <span  class="math">\(\sigma\)</span>.</p>

<p>Ok now we have our sampler and our proposal distribution, lets test this out on a very simple 1-D Gaussian likelihood function with an unknown mean and unit variance. We will use a uniform prior on the mean <span  class="math">\(\mu \sim U(-10, 10)\)</span>. Of course, we don't actually need MCMC to sampler this distribution but it works as a good example that is easy to visualize.</p>

<pre><code class="language-python">def simple_gaussian_lnpost(x):
    &quot;&quot;&quot;
    1-D Gaussian distribution with mean 0 std 1.

    Prior on mean is U(-10, 10)

    :param x: Array of parameters

    :returns: log-posterior

    &quot;&quot;&quot;
    mu = 0
    std = 1

    if x &lt; 10 and x &gt; -10:
        return -0.5 * (x-mu)**2 / std**2
    else:
        return -1e6
</code></pre>

<h2 id="visualizing-the-mhstep">Visualizing the MH-step</h2>

<p>Before we run a long chain, lets try to visualize what it going on at each iteration.</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_12_0.png" alt="png"></figure></p>

<p>The plot above shows a schematic of a successful jump. The blue shows the 1-D Gaussian posterior distribution, the orange is the Gaussian proposal distribution, <span  class="math">\(q(x_*|x_0)\)</span>, (<span  class="math">\(\sigma=0.5\)</span>), and the green and red points are the initial and proposed parameters, respectively. For illustration purposes, the gray curve shows <span  class="math">\(q(x_0|x_*)\)</span> to show the symmetry of the proposal distribution. In this case we see that the proposed point returns a Hastings ratio of 2.2 (i.e. transition probability of 1) and therefore the jump will be accepted.</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_14_0.png" alt="png"></figure></p>

<p>In this plot, we show what happens when a jump is proposed to a lower probability region. The transition probability is equal to the Hastings ratio here (remember transition probability is <span  class="math">\(\mathrm{min}(1, H)\)</span>) which is 0.32, which means that we will move to this new point with 32% probability. This ability to move back down the posterior distribution is what allows MCMC to sample the full probability distribution instead of just finding the global maximum.</p>

<p>As I promised above, lets give a quick demonstration of what happens with a <em>non-symmetric</em> proposal distribution. Lets suppose we have the same posterior but now we use a proposal from a fixed Gaussian distribution (i.e. the mean does not change with current position).</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_16_0.png" alt="png"></figure></p>

<p>In the above plot we show that the proposal distribution is a fixed Gaussian <span  class="math">\(q(x_*|x_0)\sim \textrm{Normal(-1, 1)}\)</span>. Here we show that even though the proposed point is at a <em>lower</em> posterior probability than the initial point, the Hastings ratio is still <span  class="math">\(> 1\)</span> (will accept jump with 100% probability). Qualitatively this makes sense because we need to weight that proposed point higher to take into account for the fact that the proposed point is &quot;hard&quot; to get to even though it still has a relatively high posterior probability value.</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_18_0.png" alt="png"></figure></p>

<p>In this last example, we show the opposite effect. In this case, even though the posterior probabilities are the same for the current and proposed points, the Hastings ratio is only 0.09 (i.e. 9% chance of accepting jump). Again, with some thought this makes sense. The proposed point must be weighted down because it is near the peak of the proposal distribution (i.e. lots of points will be proposed around this position) and therefore is &quot;easy&quot; to get to even though the posterior probability is no different than at the initial point.</p>

<p>In many cases we do not need to worry about the proposal densities as many proposal distributions are symmetric but you always need to keep it in mind when constructing your own jump proposals as it can lead to biased results if not correctly taken into account. I will show how to easily check for the correct ratios later in this series.</p>

<h1 id="diagnosing-efficiency-and-convergence">Diagnosing efficiency and convergence</h1>

<p>Now that we understand the Metropolis Hastings step. Lets actually run our sampler on the simple 1-D gaussian likelihood. Since this is only a 1-D problem it is quite easy to determine the optimal jump size but lets explore different values for illustration purposes.</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_22_0.png" alt="png"></figure></p>

<p>The panels are (from top left to bottom right): Histogram of samples, sample vs iteration (i.e. trace plot), log-posterior vs iteration, and proposal acceptance rate vs iteration.</p>

<p>In an optimally performing MCMC, the histogram of samples should converge to the posterior distribution, the trace of the chain should sample around the maximum of the posterior such that the samples are close to i.i.d (independent, identical distribution). The log-posterior chain should be smoothly varying around the maximum. Lastly, the acceptance rate depends on the problem but typically for 1-d problems, the acceptance rate should be around 44% (around 23% for more than 5 parameters). There are more sophisticated ways of diagnosing convergence (it gets harder with many parameters) which we will get into in further posts, but plots like this get you a long way to diagnosing problems with your sampler.</p>

<p>In this example we have used a jump standard deviation of 0.01 (remember the standard deviation of the posterior) is 1.0. So, it is clear that the histogram of samples is very poorly reproducing the posterior, the chain trace has large timescale (iteration scale) correlations (i.e. it very slowly varies around the mean). We can also see that around the middle of the run, the chain drifted off from the high probability region (bottom left plot) and it took quite a large number of samples to get back because of the small jump size. Lastly, we can see that the acceptance rate is 99%.</p>

<p>Overall, if you see something like this, the first step is to increase the jump proposal size.</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_24_0.png" alt="png"></figure></p>

<p>Ok, so <span  class="math">\(\sigma=0.01\)</span> was too small, what about <span  class="math">\(\sigma=500\)</span>? Here we see the opposite of the first example. The jump size here is way too big. While we did do a better job at recovering the posterior, we can see from the choppiness of the trace plots and low acceptance rate that this run is very inefficient as it spends a lot of time at fixed locations in parameter space and rarely moves.</p>

<p>If you see something like this, the first thing to do is to <em>decrease</em> the jump proposal size.</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_26_0.png" alt="png"></figure></p>

<p>Finally, we have a good jump proposal size. Actually this is the optimal step size for a gaussian distribution <span  class="math">\(\sigma_{\rm jump} = 2.38 \sigma_{\rm posterior} / n_{\rm dim}\)</span>. In this case we can see that the samples are near perfect draws from the posterior distribution, the chain trace is nearly i.i.d and the acceptance rate is 44%</p>

<p>The above illustrates the point but it is so easy that it may be hard to see why MCMCs are so important but difficult to implement efficiently. Before we wrap up for this week, lets look at two more, slightly difficult examples.</p>

<h1 id="a-slightly-more-complicated-problem">A (slightly) more complicated problem</h1>

<p>We have tacked a 1-D Gaussian posterior. Lets now look at a 10-D problem. This posterior distribution has the following properties</p>

<p><span  class="math">\[
\sigma^2 \sim \textrm{LogNormal}(0,1.5)\\
\mu \sim \textrm{Normal}(0, 10)
\]</span></p>

<p>That is it is a gaussian with variance drawn from a Log-Normal distribution and the means are drawn from a normal distribution with 0 mean and variance 10. This way, we can have multiple scales present in the posterior.</p>

<pre><code class="language-python">class multi_gaussian_lnpost:
    &quot;&quot;&quot;
    N-Dimensional Gaussian distribution with

    mu ~ Normal(0, 10)
    var ~ LogNormal(0, 1.5)

    Prior on mean is U(-500, 500)

    :param ndim: Numver of dimensions to gaussian (default=10)
    :param seed: Random number seed for reproducible results.

    &quot;&quot;&quot;

    def __init__(self, ndim=10, seed=12345):
        np.random.seed(seed)
        self.var = 10**(np.random.randn(ndim)*1.5)
        self.mu = scipy.stats.norm(loc=0, scale=10).rvs(ndim)

    def __call__(self, x):
        &quot;&quot;&quot;
        Call multivariate normal posterior.

        :param x: Array of parameters

        :returns: log-posterior
        &quot;&quot;&quot;

        if np.all(x &lt; 500) and np.all(x &gt; -500):
            return scipy.stats.multivariate_normal(mean=self.mu, cov=self.var).logpdf(x)
        else:
            return -1e6
</code></pre>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_31_0.png" alt="png"></figure>
<figure><img src="/img/2018-01-03-mcmc-part1/part1_31_1.png" alt="png"></figure>
<figure><img src="/img/2018-01-03-mcmc-part1/part1_31_2.png" alt="png"></figure></p>

<p>These plots are the same as those for the 1-D problem but now we show the histogram and trace for all 10 mean parameters. As before, the bottom plot shows the log-posterior vs iteration and the acceptance rate vs iteration.</p>

<p>We see that even if we have multiple scales present, we can still do fairly well with our very simple jump proposal with a single step size. However as we get to larger parameter spaces this begins to pose a problem.</p>

<p>However, if you see these kinds of results, it would be best to try different jump sizes for different parameters. For example, I will increase the jump size for parameters <span  class="math">\(x_4\)</span>, <span  class="math">\(x_5\)</span>, and <span  class="math">\(x_9\)</span>.</p>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_33_0.png" alt="png"></figure>
<figure><img src="/img/2018-01-03-mcmc-part1/part1_33_1.png" alt="png"></figure>
<figure><img src="/img/2018-01-03-mcmc-part1/part1_33_2.png" alt="png"></figure></p>

<p>OK, not perfect, but better. As you can imagine, this can get quite tedious when you have larger parameter spaces and more complicated posterior distributions. In later posts we will discuss how to automate this tuning process via adaptive jump proposals.</p>

<p>To wrap out this week though lets move to a slightly more realistic example: non-trivial covariance matrix and larger parameter space.</p>

<h1 id="covariant-parameters-in-high-dimensions">Covariant parameters in high dimensions</h1>

<p>This scenario is more realistic. In many cases, parameters have some covariance with one another and fancy techniques and tricks are really needed in high dimensions.</p>

<p>Here we will sample from a 50-dimensional gaussian distribution with non-trivial covariance matrix.</p>

<pre><code class="language-python">class corr_gaussian_lnpost:
    &quot;&quot;&quot;
    N-D Gaussian distribution with means drawn from

    mu ~ Normal(0, 1)

    and non-trivial dense covariance matrix.

    Prior on mean is U(-50, 50)

    :param ndim: Dimension of multivariate gaussian (default=10)
    :param seed: Random number seed for reproducible distribution

    &quot;&quot;&quot;

    def __init__(self, ndim=50, seed=12343):

        np.random.seed(seed)
        self.mu = np.random.rand(ndim)

        M = np.random.randn(ndim, ndim)
        self.cov = np.dot(M, M.T)

    def __call__(self, x):
        &quot;&quot;&quot;Return multivariate-normal log posterior

        :param x: Array of parameters

        :returns: log-posterior
        &quot;&quot;&quot;

        if np.all(x &lt; 50) and np.all(x &gt; -50):
            return scipy.stats.multivariate_normal(mean=self.mu, cov=self.cov).logpdf(x)
        else:
            return -1e6
</code></pre>

<p><figure><img src="/img/2018-01-03-mcmc-part1/part1_38_1.png" alt="png"></figure>
<figure><img src="/img/2018-01-03-mcmc-part1/part1_38_2.png" alt="png"></figure>
<figure><img src="/img/2018-01-03-mcmc-part1/part1_38_3.png" alt="png"></figure>
<figure><img src="/img/2018-01-03-mcmc-part1/part1_38_4.png" alt="png"></figure></p>

<p>Above, we only plot the first 10 parameters but it is obvious that, even after 1 million samples, our gaussian jump proposal is not working very well at all as the chains have not converged.</p>

<p>We will pick up with this problem next week when we introduce adaptive jump proposals and jump schedules.</p>

      </div>

      




    </div>
  </div>

</article>






<div class="article-container">
  
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "jellis18-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Justin A. Ellis &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//jellis18-github-io.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

